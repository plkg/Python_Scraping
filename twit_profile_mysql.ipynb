{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver-manager\n",
    "!pip install selenium\n",
    "!pip install beautifulsoup4 requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import mysql.connector\n",
    "\n",
    "import time\n",
    "time.sleep(5)  # Wait 5 seconds between requests\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "def setup_driver(driver_path):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--verbose\")\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    service = Service(driver_path, log_path=\"chromedriver.log\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    print(f\"Using ChromeDriver from: {driver_path}\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "# Function to scrape Twitter profile information\n",
    "def scrape_twitter_profile(driver, profile_url):\n",
    "    # Open Twitter profile page\n",
    "    driver.get(profile_url)\n",
    "    \n",
    "    try:\n",
    "        # Wait for bio to load\n",
    "        bio_element = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='UserDescription']\"))\n",
    "        )\n",
    "        bio = bio_element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching bio for {profile_url}: {e}\")\n",
    "        bio = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        # Wait for following count to load\n",
    "        following_count_element = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/following')]//span\"))\n",
    "        )\n",
    "        following_count = following_count_element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching following count for {profile_url}: {e}\")\n",
    "        following_count = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        # Wait for followers count to load\n",
    "        followers_count_element = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/followers')]//span\"))\n",
    "        )\n",
    "        followers_count = followers_count_element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching followers count for {profile_url}: {e}\")\n",
    "        followers_count = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        # Wait for location to load\n",
    "        location_element = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//span[@data-testid='UserLocation']\"))\n",
    "        )\n",
    "        location = location_element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching location for {profile_url}: {e}\")\n",
    "        location = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        # Wait for website to load\n",
    "        website_element = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[@data-testid='UserUrl']\"))\n",
    "        )\n",
    "        website = website_element.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching website for {profile_url}: {e}\")\n",
    "        website = \"N/A\"\n",
    "\n",
    "    return {\n",
    "        \"Bio\": bio,\n",
    "        \"Following Count\": following_count,\n",
    "        \"Followers Count\": followers_count,\n",
    "        \"Location\": location,\n",
    "        \"Website\": website,\n",
    "        \"Profile URL\": profile_url\n",
    "    }\n",
    "\n",
    "# Function to insert scraped data into MySQL\n",
    "def insert_data_to_mysql(db_config, profile_data):\n",
    "    conn = None  # Initialize the connection variable\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = mysql.connector.connect(\n",
    "            host=db_config[\"host\"],\n",
    "            user=db_config[\"user\"],\n",
    "            password=db_config[\"password\"],\n",
    "            database=db_config[\"database\"]\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the table\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO twitter_profiles (Bio, Following_Count, Followers_Count, Location, Website, Profile_URL)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        data_tuple = (\n",
    "            profile_data[\"Bio\"],\n",
    "            profile_data[\"Following Count\"],\n",
    "            profile_data[\"Followers Count\"],\n",
    "            profile_data[\"Location\"],\n",
    "            profile_data[\"Website\"],\n",
    "            profile_data[\"Profile URL\"]\n",
    "        )\n",
    "        cursor.execute(insert_query, data_tuple)\n",
    "        conn.commit()\n",
    "        print(f\"Data inserted successfully for {profile_data['Profile URL']}\")\n",
    "\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        # Close the database connection if it was successfully opened\n",
    "        if conn is not None and conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "# Main function to handle the workflow\n",
    "def main(csv_filename, driver_path, db_config):\n",
    "    # Step 1: Read the already-downloaded CSV file with Twitter profile URLs\n",
    "    with open(csv_filename, newline='', encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        profile_urls = [row[0] for row in reader]  # Assuming each row has one URL\n",
    "\n",
    "    # Step 2: Initialize Selenium WebDriver\n",
    "    driver = setup_driver(driver_path)\n",
    "\n",
    "    # Step 3: Scrape data and insert into MySQL\n",
    "    for url in profile_urls:\n",
    "        print(f\"Scraping data for: {url}\")\n",
    "        profile_data = scrape_twitter_profile(driver, url)\n",
    "        insert_data_to_mysql(db_config, profile_data)\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the already-downloaded CSV file\n",
    "    csv_filename = \"twitter_profiles.csv\"  # Replace with the actual path to your local CSV file\n",
    "\n",
    "    # Path to Chrome WebDriver\n",
    "    driver_path = r\"E:\\Deep_Learning\\Scraping\\chromedriver-win64\\chromedriver.exe\"  # Replace with the actual path to your WebDriver\n",
    "\n",
    "    # MySQL database configuration\n",
    "    db_config = {\n",
    "        \"host\": \"localhost\",\n",
    "        \"user\": \"root\",  # Replace with your MySQL username\n",
    "        \"password\": \"root\",  # Replace with your MySQL password\n",
    "        \"database\": \"twitter_scraper_db\"  # Replace with your MySQL database\n",
    "    }\n",
    "\n",
    "    main(csv_filename, driver_path, db_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
